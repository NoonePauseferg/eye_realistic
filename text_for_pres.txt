[Cлайд 2]
В своей работе я задавался вопросом использования синтетических
данных для обучения нейросетевых моделей. Преимущества такого
подхода понятны, синтетические данные легко генерировать и их
не нужно размечать.

[Слайд 3]
Здесь я привожу пример сэмплов реальных и синтетических данных.
Синтетические данные, которые были получены с помощью симулятора
от unity, выглядят очень чисто. Но если сравить их с реальными данными,
то можно заметить большую разницу. Напрашивается очевидный вопрос : 
Почему мы не можем сразу генерировать изображения достаточно реалистичными?
Дело в вычислительных мощностях и стоимости, даже если иметь все что нужно,
не будет ни каких гарантий, что симмулятор впринцепи сможет выдать реалистичную 
картинку.

[Слайд 4]
И собственно чем я занимался, я занимался уменьшением разницы между распределениями
синтетических и реальных данных.

Дальше я расскажу про подход, который описан в статье от компании apple и про то,
что получилось у меня. Небольшая предыстория, зачем это исследование apple. Возможно
вы уже видели гаджет, который называется умные очки. Для его функционирования нужно
точно понимать куда смотрит пользователь, детектировать каждое движение его зрачка,
для этого и проводилось это исследование, чтобы нагенерить много данных для обучения.

Цель подхода заключается в том, чтобы обучить нейронную сеть, которая работала бы с синтетическими
изображениями и выдавала более реалистичные их версии.

[Слайд 5]
Начинка подхода заключается в архитектуре под названием SimGAN.
Она состоит из двух частей, refiner и дискриминатора. 

    Дискриминатор это классификатор на два класса, обычная fully-conv сеть. 
Его задача отличать реальные изображения от сгенерированных.
    Задача refiner-a, представляющего из себя resnet архитектуру, обмануть 
дискримианатор и генерировать изображения, так, чтобы дискриминатор не мог их отличить
от настоящих. 
    Важный поинт в том, что мы хотим, чтобы
сгенерированные изображения имели те же натуральные параметры, что и синтетические,
я имею ввиду форму глаз, ресницы, зрачки; надеюсь понятно. Поэтому в лоссе refiner-a
есть добавка под названием self-regularization loss, которая представляет из себя
обычный L1 лосс между исходным и сгенерированным изображениями. Остальня часть лосса
просто отвечает за то, на сколько хорошо получается обмануть дискриминатор.

[Слайд 6]
Я реализовал эту несложную архитектуру. В качестве синтетических данных я использовал
датасет Eye Gaze, а для реальных - датасет Helen Eye. Все данные я нашел на kaggle.
Как можно заметить, изображения дествительно стали реалистичнее и если дать человеку
пару сгенерированного и синтетического изображения, то он сможет достаточно хорошо отличить
их.

[Слайд 7]
Улучшения в модели. 
    Важным требованием к генератору состоит в том, чтобы сгенерированные
изображения были без артефактов. Любой участок сгенерированного изображения должен иметь
те же статистики, что и реальное. Поэтому мы можем определить дискриминатор так, чтобы
он классифицировал каждый участок отдельно, на выходе будет матрица вероятностей. Такой
подход не только уменьшает receptive field модели, уменьшая ее сложность, но и работает с множеством
сэмплов изображения чтобы лучше натренировать дискриминатор.

    Другая проблема, состоит в том, что дискриминатор фокусируется на последних сгенерированных изображениях.
Это может привести к расхождению между трейном и валидацией, повторному введению артефактов, о которых дискриминатор
забыл. Поэтому вводится механизм, связанный с историей сгенерированных изображений. Используется буффер
сгенерированных изображений фиксированного размера, например размера батча. И на каждой итерации сэмплируем
половину данных из текущего батча, а половину из истории. После случайно заменяем половину изображений в буффере
на изображения из текущего батча.

[Слайд 8]
    Вот что получилось с использованием улучшений. Как можно заметить синтетические изображения очень сильно
стали похожими на реальные. Тут я привел часть логов с обучения и валидации, как видно, мне удалось добиться
совместной работы генератора и дискриминатора, что было не просто)